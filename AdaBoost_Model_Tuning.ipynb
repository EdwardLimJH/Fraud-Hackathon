{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read every column except 'device_fraud_count' as its value is a constant 0\n",
    "df = pd.read_csv('Base.csv', usecols=lambda x: x != 'device_fraud_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handle Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with missing values represented by negative values according to documentation\n",
    "missing_features = ['prev_address_months_count', 'current_address_months_count', 'intended_balcon_amount', \n",
    "                    'bank_months_count', 'session_length_in_minutes', 'device_distinct_emails_8w']\n",
    "\n",
    "# Replace negative values with NaN\n",
    "for feature in missing_features:\n",
    "    df[feature] = df[feature].apply(lambda x: x if x >= 0 else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encode missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['prev_address_months_count', 'intended_balcon_amount', 'bank_months_count']\n",
    "for col in features_to_drop:\n",
    "    missing_column_name = f'{col}_missing'\n",
    "    df[missing_column_name] = np.where(df[col].isna(), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop features with a high percentage of missing values, and have very weak correlation with fraud status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(features_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows with missing values as a very small percentage of the remaining observations have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handle Categorical Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform dummy encoding. Very similar to one-hot encoding, but the first encoded column is dropped to reduce correlation between encoded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only features with String data type need to be encoded\n",
    "encoded_features = [feature for feature in df.columns if df[feature].dtype == 'object']\n",
    "\n",
    "df = pd.get_dummies(df, columns=encoded_features, drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the feature matrix and target variable\n",
    "X = df.drop('fraud_bool', axis=1)\n",
    "y = df['fraud_bool']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-Max Scaling (Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EDA, numerical features were identified. Min-max scaling is applied as parametric models are sensitive to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['income', 'name_email_similarity', 'current_address_months_count', 'customer_age', 'days_since_request', 'zip_count_4w', 'velocity_6h', 'velocity_24h', \n",
    "                    'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score', 'proposed_credit_limit', 'session_length_in_minutes']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit only on the training data\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Selection - Backward Stepwise (logistic model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# def backward_stepwise_selection(X, y, p_threshold=0.05):\n",
    "#     features = X.columns.tolist()\n",
    "#     num_features = len(features)\n",
    "    \n",
    "#     for i in range(num_features, 0, -1):\n",
    "#         model = sm.Logit(y, X[features]).fit()\n",
    "#         p_values = model.pvalues\n",
    "#         max_p_value = p_values.max()\n",
    "#         if max_p_value > p_threshold:\n",
    "#             remove_feature = p_values.idxmax()\n",
    "#             print(f\"Removing '{remove_feature}' with p-value: {max_p_value:.4f}\")\n",
    "#             features.remove(remove_feature)\n",
    "#         else:\n",
    "#             break\n",
    "            \n",
    "#     return features\n",
    "\n",
    "# selected_features = backward_stepwise_selection(X_train, y_train)\n",
    "# print(\"Selected Features:\", selected_features) #35 features\n",
    "# #['income', 'name_email_similarity', 'customer_age', 'zip_count_4w', 'velocity_6h', 'velocity_24h', \n",
    "# # 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score', \n",
    "# # 'email_is_free', 'phone_home_valid', 'phone_mobile_valid', 'has_other_cards', 'proposed_credit_limit',\n",
    "# # 'foreign_request', 'session_length_in_minutes', 'keep_alive_session', 'device_distinct_emails_8w', 'month',\n",
    "# # 'payment_type_AC', 'employment_status_CB', 'employment_status_CC', 'employment_status_CD', 'employment_status_CE',\n",
    "# # 'employment_status_CF', 'housing_status_BB', 'housing_status_BC', 'housing_status_BD', 'housing_status_BE', 'housing_status_BF',\n",
    "# # 'source_TELEAPP', 'device_os_macintosh', 'device_os_windows', 'device_os_x11']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features = ['income', 'name_email_similarity', 'customer_age', 'zip_count_4w', 'velocity_6h', 'velocity_24h', \n",
    "# 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score', \n",
    "# 'email_is_free', 'phone_home_valid', 'phone_mobile_valid', 'has_other_cards', 'proposed_credit_limit',\n",
    "# 'foreign_request', 'session_length_in_minutes', 'keep_alive_session', 'device_distinct_emails_8w', 'month',\n",
    "# 'payment_type_AC', 'employment_status_CB', 'employment_status_CC', 'employment_status_CD', 'employment_status_CE',\n",
    "# 'employment_status_CF', 'housing_status_BB', 'housing_status_BC', 'housing_status_BD', 'housing_status_BE', 'housing_status_BF',\n",
    "# 'source_TELEAPP', 'device_os_macintosh', 'device_os_windows', 'device_os_x11'] + ['prev_address_months_count_missing', 'intended_balcon_amount_missing', 'bank_months_count_missing']\n",
    "\n",
    "# # X_train = X_train[selected_features]\n",
    "# # X_test = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated features\n",
    "selected_features = ['income', 'name_email_similarity', 'customer_age', \n",
    "                     'zip_count_4w', 'velocity_6h', 'velocity_24h', \n",
    "                     'velocity_4w', 'bank_branch_count_8w', \n",
    "                     'date_of_birth_distinct_emails_4w', 'credit_risk_score', \n",
    "                     'email_is_free', 'phone_home_valid', 'phone_mobile_valid',\n",
    "                     'has_other_cards', 'proposed_credit_limit', 'foreign_request', \n",
    "                     'keep_alive_session', 'month', 'payment_type_AC', 'employment_status_CB', \n",
    "                     'employment_status_CC', 'employment_status_CD', 'employment_status_CE', \n",
    "                     'employment_status_CF', 'housing_status_BB', 'housing_status_BC', \n",
    "                     'housing_status_BD', 'housing_status_BE', 'housing_status_BF', \n",
    "                     'source_TELEAPP', 'device_os_macintosh', 'device_os_windows', \n",
    "                     'device_os_x11', 'prev_address_months_count_missing', \n",
    "                     'intended_balcon_amount_missing']\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resampling**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraud class vs non fraud class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of non-fraud class in y: 98.893%\n",
      "% of fraud class in y: 1.107%\n",
      "\n",
      "% of non-fraud class in y_train: 98.893%\n",
      "% of fraud class in y_train: 1.107%\n",
      "\n",
      "% of non-fraud class in y_test: 98.893%\n",
      "% of fraud class in y_test: 1.107%\n"
     ]
    }
   ],
   "source": [
    "ratio = y.value_counts() / len(y) * 100\n",
    "print(f'% of non-fraud class in y: {round(ratio[0],3)}%\\n% of fraud class in y: {round(ratio[1],3)}%\\n')\n",
    "\n",
    "ratio_train = y_train.value_counts() / len(y_train) * 100\n",
    "print(f'% of non-fraud class in y_train: {round(ratio_train[0],3)}%\\n% of fraud class in y_train: {round(ratio_train[1],3)}%\\n')\n",
    "\n",
    "ratio_test = y_test.value_counts() / len(y_test) * 100\n",
    "print(f'% of non-fraud class in y_test: {round(ratio_test[0],3)}%\\n% of fraud class in y_test: {round(ratio_test[1],3)}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of non-fraud class in resampled data: 60.024%\n",
      "% of fraud class in resampled data: 39.976%\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy = 0.666) #ratio of minority:majority 40:60\n",
    "\n",
    "Xt_resampled_SMOTE, yt_resampled_SMOTE = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "ratio_SMOTE = yt_resampled_SMOTE.value_counts() / len(yt_resampled_SMOTE) * 100\n",
    "print(f'% of non-fraud class in resampled data: {round(ratio_SMOTE[0],3)}%\\n% of fraud class in resampled data: {round(ratio_SMOTE[1],3)}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names = ['Ratio of Classes', 'Accuracy', 'Recall','Precision', 'F2 Score', 'F1.5 Score','F1 Score', \n",
    "                 'TPR','FNR', \"PR-AUC\", 'Balanced Accuracy', 'Kappa Statistic']\n",
    "results = pd.DataFrame(index= metrics_names,columns=['Original Dataset', 'SMOTE'])\n",
    "class_reports = {}\n",
    "pr_auc_pts = {}\n",
    "\n",
    "results.loc['Ratio of Classes','Original Dataset'] = str(round(ratio_train,3)[0]) + '% : ' +str(round(ratio_train,3)[1])+'%'\n",
    "results.loc['Ratio of Classes','SMOTE'] = str(round(ratio_SMOTE,3)[0]) + '% : ' +str(round(ratio_SMOTE,3)[1])+'%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, fbeta_score, f1_score, average_precision_score, precision_recall_curve, confusion_matrix,balanced_accuracy_score, cohen_kappa_score\n",
    "# def evaluate_results(model,resampler,x_resampled, y_resampled):\n",
    "\n",
    "#     model.fit(x_resampled, y_resampled)\n",
    "\n",
    "#     y_pred_test = model.predict(X_test)\n",
    "\n",
    "#     results.loc['Accuracy',resampler] = accuracy_score(y_test, y_pred_test)\n",
    "#     class_reports[resampler] = classification_report(y_test, y_pred_test)\n",
    "#     results.loc['Recall',resampler] = recall_score(y_test, y_pred_test)\n",
    "#     results.loc['Precision',resampler] = precision_score(y_test, y_pred_test)\n",
    "#     results.loc['F2 Score',resampler] = fbeta_score(y_test, y_pred_test, beta =2)\n",
    "#     results.loc['F1.5 Score',resampler] = fbeta_score(y_test, y_pred_test, beta =1.5)\n",
    "#     results.loc['F1 Score',resampler] = f1_score(y_test, y_pred_test)\n",
    "#     results.loc['PR-AUC',resampler] = average_precision_score(y_test, y_pred_test)\n",
    "#     pr_auc_pts[resampler] = precision_recall_curve(y_test, y_pred_test)\n",
    "#     results.loc['Balanced Accuracy',resampler] = balanced_accuracy_score(y_test, y_pred_test)\n",
    "#     results.loc['Kappa Statistic',resampler] = cohen_kappa_score(y_test, y_pred_test)\n",
    "    \n",
    "#     cm = confusion_matrix(y_test, y_pred_test, labels=[0,1])\n",
    "#     TN, FP, FN, TP = cm.ravel()\n",
    "#     TPR = TP/(TP+FN)\n",
    "#     FNR = FN/(TP+FN)\n",
    "#     results.loc['TPR',resampler] = TPR\n",
    "#     results.loc['FNR',resampler] = FNR\n",
    "\n",
    "#     print(f\"{resampler} Model Performance on Test Data:\")\n",
    "#     print(f\"{resampler} Accuracy:\", results.loc['Accuracy',resampler])\n",
    "#     print(f\"{resampler} Precision: {results.loc['Precision',resampler]}\")\n",
    "#     print(f\"{resampler} Recall: {results.loc['Recall',resampler]}\")\n",
    "#     print(f\"{resampler} F2: {results.loc['F2 Score',resampler]}\")\n",
    "#     print(f\"{resampler} F1.5: {results.loc['F1.5 Score',resampler]}\")\n",
    "#     print(f\"{resampler} F1: {results.loc['F1 Score',resampler]}\")\n",
    "#     print(f\"{resampler} PR-AUC: {results.loc['PR-AUC',resampler]}\")\n",
    "#     print(f\"{resampler} TPR: {results.loc['TPR',resampler]}\")\n",
    "#     print(f\"{resampler} FNR: {results.loc['FNR',resampler]}\")\n",
    "#     print(f\"{resampler} Balanced Accuracy: {results.loc['Balanced Accuracy',resampler]}\")\n",
    "#     print(f\"{resampler} Kappa Statistic: {results.loc['Kappa Statistic',resampler]}\")\n",
    "#     print(f\"{resampler} Classification Report: \\n{class_reports[resampler]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## To use function\\nxgb_base_pred = xgb_base.predict(X_test_new)\\n\\nxgb_base_results = evaluate_results(y_test=y_test_new, y_pred=xgb_base_pred)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, fbeta_score, f1_score, average_precision_score, precision_recall_curve, confusion_matrix,balanced_accuracy_score, cohen_kappa_score\n",
    "def evaluate_results(y_test, y_pred):\n",
    "    score_results = {}\n",
    "    score_results[\"accuracy_score\"] = accuracy_score(y_test, y_pred)\n",
    "    score_results[\"classification_report\"] = classification_report(y_test, y_pred)\n",
    "    score_results[\"recall_score\"] = recall_score(y_test, y_pred)\n",
    "    score_results[\"precision_score\"] = precision_score(y_test, y_pred)\n",
    "    score_results[\"F2-score\"] = fbeta_score(y_test, y_pred, beta =2)\n",
    "    score_results[\"F1-score\"] = f1_score(y_test, y_pred)\n",
    "    score_results[\"average_precision_score\"] = average_precision_score(y_test, y_pred)\n",
    "    score_results[\"PR-AUC\"] = precision_recall_curve(y_test, y_pred)\n",
    "    score_results[\"balanced_accuracy_score\"] = balanced_accuracy_score(y_test, y_pred)\n",
    "    score_results[\"Kappa statistics\"] = cohen_kappa_score(y_test,y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    TPR = TP/(TP+FN)\n",
    "    FNR = FN/(TP+FN)\n",
    "    score_results[\"TPR\"] = TPR\n",
    "    score_results[\"FNR\"] = FNR\n",
    "    return score_results\n",
    "\n",
    "'''\n",
    "## To use function\n",
    "xgb_base_pred = xgb_base.predict(X_test_new)\n",
    "\n",
    "xgb_base_results = evaluate_results(y_test=y_test_new, y_pred=xgb_base_pred)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## Sample usage\\nsave_model_and_results(clf_smote_encoded,\"C:/NUS/Fraud-Hackathon/models/baseline_encoded.pkl\", models_smote_encoded, \"C:/NUS/Fraud-Hackathon/models/baseline_encoded_results.csv\")\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model, model_filename):\n",
    "    pickle.dump(model, open(model_filename,\"wb\"))\n",
    "\n",
    "# This function is more spcific to the results from lazy classifier. feel free to overwrite it\n",
    "def save_results(results_df, results_filename):\n",
    "    results_df = results_df[\"evaluate_results\"].reset_index()\n",
    "    # Convert the 'Metrics' column into separate columns\n",
    "    df_metrics = pd.json_normalize(results_df['evaluate_results'])\n",
    "\n",
    "    # Concatenate the two DataFrames\n",
    "    results_df = pd.concat([results_df['Model'].rename('Model'), df_metrics], axis=1)\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "def save_model_and_results(model, model_filename, results_df, results_filename):\n",
    "    save_model(model, model_filename)\n",
    "    save_results(results_df, results_filename)\n",
    "\n",
    "'''\n",
    "## Sample usage\n",
    "save_model_and_results(clf_smote_encoded,\"C:/NUS/Fraud-Hackathon/models/baseline_encoded.pkl\", models_smote_encoded, \"C:/NUS/Fraud-Hackathon/models/baseline_encoded_results.csv\")\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\dsa4263\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy_score': 0.9379786837894144,\n",
       " 'classification_report': '              precision    recall  f1-score   support\\n\\n           0       0.99      0.94      0.97    196523\\n           1       0.09      0.48      0.15      2199\\n\\n    accuracy                           0.94    198722\\n   macro avg       0.54      0.71      0.56    198722\\nweighted avg       0.98      0.94      0.96    198722\\n',\n",
       " 'recall_score': 0.4756707594361073,\n",
       " 'precision_score': 0.08561139302668194,\n",
       " 'F2-score': 0.24888169791567527,\n",
       " 'F1-score': 0.14510647152666992,\n",
       " 'average_precision_score': 0.046524911598302125,\n",
       " 'PR-AUC': (array([0.01106571, 0.08561139, 1.        ]),\n",
       "  array([1.        , 0.47567076, 0.        ]),\n",
       "  array([0, 1], dtype=int64)),\n",
       " 'balanced_accuracy_score': 0.709411225802227,\n",
       " 'Kappa statistics': 0.12876581867168801,\n",
       " 'TPR': 0.4756707594361073,\n",
       " 'FNR': 0.5243292405638926}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without tuning\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost = AdaBoostClassifier(random_state=42)\n",
    "adaboost.fit(Xt_resampled_SMOTE, yt_resampled_SMOTE)\n",
    "adaboost_pred = adaboost.predict(X_test)\n",
    "evaluate_results(y_test, adaboost_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 88320\n",
      "max_resources_: 794885\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 16\n",
      "n_resources: 88320\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 6\n",
      "n_resources: 264960\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 794880\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\dsa4263\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results: {'iter': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       2, 2]), 'n_resources': array([ 88320,  88320,  88320,  88320,  88320,  88320,  88320,  88320,\n",
      "        88320,  88320,  88320,  88320,  88320,  88320,  88320,  88320,\n",
      "       264960, 264960, 264960, 264960, 264960, 264960, 794880, 794880]), 'mean_fit_time': array([ 13.556599  ,  26.10086598,  55.5783154 , 126.84125729,\n",
      "        16.4772531 ,  33.20272679,  67.17661357, 131.64660969,\n",
      "        18.24864426,  37.14128652,  59.31608024, 111.96014886,\n",
      "        14.64974298,  28.24164906,  56.32459803,  90.92751551,\n",
      "       196.7778141 , 398.89205589, 401.22199583,  53.63603668,\n",
      "       182.139745  ,  79.75948324, 431.28253202, 584.49625034]), 'std_fit_time': array([6.88420449e-01, 1.27801033e+00, 2.89205587e+00, 5.34333164e+00,\n",
      "       5.61044643e-01, 2.75529492e+00, 1.21655073e+00, 2.55355384e+00,\n",
      "       1.65621594e+00, 7.65745068e-01, 1.87519317e+00, 7.50697106e-01,\n",
      "       7.24304580e-02, 5.66549715e-01, 3.11140714e-01, 6.08290736e+00,\n",
      "       1.83027982e+00, 3.19298486e+00, 3.63650649e+00, 3.16321550e+00,\n",
      "       4.22543943e+00, 1.28875476e+01, 6.82022421e+00, 1.29524325e+02]), 'mean_score_time': array([0.19384289, 0.34457698, 0.81972532, 1.55314326, 0.22965193,\n",
      "       0.41712189, 0.75330606, 1.5187788 , 0.23715382, 0.47271919,\n",
      "       0.69085412, 1.37809138, 0.20807314, 0.3527936 , 0.67883677,\n",
      "       0.78475132, 2.49930038, 4.59419837, 5.18887215, 0.68128395,\n",
      "       2.34259353, 0.71584029, 4.13563471, 4.55317135]), 'std_score_time': array([0.04205203, 0.0313104 , 0.10427375, 0.12409304, 0.03618425,\n",
      "       0.05562169, 0.04369961, 0.15509334, 0.02866765, 0.08030462,\n",
      "       0.02147839, 0.06447684, 0.01675827, 0.01480917, 0.02131608,\n",
      "       0.1701133 , 0.10898175, 0.14780691, 1.4429246 , 0.05709873,\n",
      "       0.22145386, 0.18359378, 0.32207249, 1.14137561]), 'param_adaboostclassifier__learning_rate': masked_array(data=[0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
      "                   0.1, 0.1, 0.1, 0.1, 1, 1, 1, 1, 0.1, 0.1, 1, 1, 1, 1,\n",
      "                   1, 1],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_adaboostclassifier__n_estimators': masked_array(data=[25, 50, 100, 200, 25, 50, 100, 200, 25, 50, 100, 200,\n",
      "                   25, 50, 100, 200, 100, 200, 200, 25, 100, 50, 50, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'adaboostclassifier__learning_rate': 0.001, 'adaboostclassifier__n_estimators': 25}, {'adaboostclassifier__learning_rate': 0.001, 'adaboostclassifier__n_estimators': 50}, {'adaboostclassifier__learning_rate': 0.001, 'adaboostclassifier__n_estimators': 100}, {'adaboostclassifier__learning_rate': 0.001, 'adaboostclassifier__n_estimators': 200}, {'adaboostclassifier__learning_rate': 0.01, 'adaboostclassifier__n_estimators': 25}, {'adaboostclassifier__learning_rate': 0.01, 'adaboostclassifier__n_estimators': 50}, {'adaboostclassifier__learning_rate': 0.01, 'adaboostclassifier__n_estimators': 100}, {'adaboostclassifier__learning_rate': 0.01, 'adaboostclassifier__n_estimators': 200}, {'adaboostclassifier__learning_rate': 0.1, 'adaboostclassifier__n_estimators': 25}, {'adaboostclassifier__learning_rate': 0.1, 'adaboostclassifier__n_estimators': 50}, {'adaboostclassifier__learning_rate': 0.1, 'adaboostclassifier__n_estimators': 100}, {'adaboostclassifier__learning_rate': 0.1, 'adaboostclassifier__n_estimators': 200}, {'adaboostclassifier__learning_rate': 1, 'adaboostclassifier__n_estimators': 25}, {'adaboostclassifier__learning_rate': 1, 'adaboostclassifier__n_estimators': 50}, {'adaboostclassifier__learning_rate': 1, 'adaboostclassifier__n_estimators': 100}, {'adaboostclassifier__learning_rate': 1, 'adaboostclassifier__n_estimators': 200}, {'adaboostclassifier__learning_rate': 0.1, 'adaboostclassifier__n_estimators': 100}, {'adaboostclassifier__learning_rate': 0.1, 'adaboostclassifier__n_estimators': 200}, {'adaboostclassifier__learning_rate': 1, 'adaboostclassifier__n_estimators': 200}, {'adaboostclassifier__learning_rate': 1, 'adaboostclassifier__n_estimators': 25}, {'adaboostclassifier__learning_rate': 1, 'adaboostclassifier__n_estimators': 100}, {'adaboostclassifier__learning_rate': 1, 'adaboostclassifier__n_estimators': 50}, {'adaboostclassifier__learning_rate': 1, 'adaboostclassifier__n_estimators': 50}, {'adaboostclassifier__learning_rate': 1, 'adaboostclassifier__n_estimators': 100}], 'split0_test_score': array([0.08998671, 0.08998671, 0.1094196 , 0.1094196 , 0.1094196 ,\n",
      "       0.1094196 , 0.11726207, 0.14874142, 0.15588465, 0.18462124,\n",
      "       0.20951302, 0.22231371, 0.23912107, 0.25465498, 0.24333333,\n",
      "       0.24      , 0.20728257, 0.22090517, 0.20457604, 0.21003501,\n",
      "       0.22078804, 0.22963368, 0.24366239, 0.25428762]), 'split1_test_score': array([0.08373786, 0.08373786, 0.08373786, 0.09620068, 0.09620068,\n",
      "       0.09620068, 0.1086272 , 0.13368984, 0.14847667, 0.17206883,\n",
      "       0.18483591, 0.19688447, 0.2       , 0.21934758, 0.18175583,\n",
      "       0.17123288, 0.19018888, 0.2064724 , 0.19506248, 0.19875588,\n",
      "       0.22351198, 0.22940258, 0.25289188, 0.26066351]), 'split2_test_score': array([0.07586658, 0.07586658, 0.07586658, 0.07586658, 0.09433962,\n",
      "       0.09433962, 0.09433962, 0.13257576, 0.13954331, 0.17324888,\n",
      "       0.17744917, 0.20493517, 0.20309051, 0.19830028, 0.21292776,\n",
      "       0.21719858, 0.21273964, 0.22914629, 0.22581544, 0.21394163,\n",
      "       0.23493836, 0.22797738, 0.23717678, 0.24435408]), 'split3_test_score': array([0.07049474, 0.07049474, 0.0911927 , 0.0911927 , 0.0911927 ,\n",
      "       0.0911927 , 0.10115607, 0.12115732, 0.13106416, 0.16814486,\n",
      "       0.18937987, 0.21117747, 0.20166594, 0.21576525, 0.20996441,\n",
      "       0.23560209, 0.19754601, 0.22028548, 0.23767083, 0.21833733,\n",
      "       0.25959109, 0.23227836, 0.25128599, 0.26166642]), 'split4_test_score': array([0.06867112, 0.06867112, 0.09153884, 0.09153884, 0.09153884,\n",
      "       0.09153884, 0.10050963, 0.12041497, 0.13752053, 0.16586731,\n",
      "       0.18728053, 0.2166302 , 0.22317403, 0.246875  , 0.22197141,\n",
      "       0.19711538, 0.20909434, 0.22730462, 0.23392695, 0.22442049,\n",
      "       0.27227723, 0.25531915, 0.25698026, 0.26762353]), 'mean_test_score': array([0.0777514 , 0.0777514 , 0.09035112, 0.09284368, 0.09653829,\n",
      "       0.09653829, 0.10437892, 0.13131586, 0.14249786, 0.17279022,\n",
      "       0.1896917 , 0.2103882 , 0.21341031, 0.22698862, 0.21399055,\n",
      "       0.21222979, 0.20337029, 0.22082279, 0.21941035, 0.21309807,\n",
      "       0.24222134, 0.23492223, 0.24839946, 0.25771903]), 'std_test_score': array([0.00805237, 0.00805237, 0.01112891, 0.01075807, 0.00669994,\n",
      "       0.00669994, 0.00787615, 0.01032374, 0.00870844, 0.00648398,\n",
      "       0.01069817, 0.00887569, 0.01536645, 0.02082434, 0.01990532,\n",
      "       0.02550031, 0.00828888, 0.00796899, 0.01672171, 0.00861904,\n",
      "       0.02033486, 0.01029262, 0.00707859, 0.00790882]), 'rank_test_score': array([23, 23, 22, 21, 19, 19, 18, 17, 16, 15, 14, 12,  9,  5,  8, 11, 13,\n",
      "        6,  7, 10,  3,  4,  2,  1]), 'split0_train_score': array([0.08264836, 0.08264836, 0.10206504, 0.10206504, 0.10206504,\n",
      "       0.10206504, 0.11425095, 0.14288427, 0.15530672, 0.18507891,\n",
      "       0.20517009, 0.22843723, 0.21937132, 0.2467551 , 0.23255814,\n",
      "       0.22476675, 0.20806059, 0.22749174, 0.22910673, 0.22611016,\n",
      "       0.23859222, 0.24731685, 0.24605344, 0.25812388]), 'split1_train_score': array([0.07701458, 0.07701458, 0.07701458, 0.10086543, 0.10086543,\n",
      "       0.10086543, 0.11825131, 0.13841505, 0.15832432, 0.19159764,\n",
      "       0.19504381, 0.21578833, 0.21348443, 0.21954674, 0.21361461,\n",
      "       0.19946809, 0.20878629, 0.22582833, 0.21098636, 0.21275024,\n",
      "       0.23676795, 0.23764077, 0.24842726, 0.26173894]), 'split2_train_score': array([0.08069829, 0.08069829, 0.08069829, 0.08069829, 0.10705861,\n",
      "       0.10705861, 0.10705861, 0.14943527, 0.15650741, 0.17539713,\n",
      "       0.19301384, 0.21036974, 0.21098459, 0.23069061, 0.24799695,\n",
      "       0.22116904, 0.20311177, 0.22618708, 0.23093761, 0.21849043,\n",
      "       0.24783897, 0.2374695 , 0.2541798 , 0.25854337]), 'split3_train_score': array([0.07620744, 0.07620744, 0.09666565, 0.09666565, 0.09666565,\n",
      "       0.09666565, 0.11793228, 0.1412615 , 0.14879739, 0.16784284,\n",
      "       0.19070768, 0.20766773, 0.19993516, 0.22927044, 0.22756005,\n",
      "       0.20973531, 0.20298883, 0.22322962, 0.23620325, 0.22512695,\n",
      "       0.25610449, 0.24329945, 0.25062638, 0.26033066]), 'split4_train_score': array([0.07886964, 0.07886964, 0.09826731, 0.09826731, 0.09826731,\n",
      "       0.09826731, 0.10787862, 0.12829946, 0.15014563, 0.18811395,\n",
      "       0.20053222, 0.21968085, 0.21858223, 0.23252209, 0.23153356,\n",
      "       0.21769178, 0.20094787, 0.21968117, 0.2210614 , 0.19759196,\n",
      "       0.24460088, 0.23945862, 0.2419559 , 0.25728567]), 'mean_train_score': array([0.07908766, 0.07908766, 0.09094217, 0.09571234, 0.10098441,\n",
      "       0.10098441, 0.11307435, 0.14005911, 0.15381629, 0.18160609,\n",
      "       0.19689353, 0.21638878, 0.21247155, 0.231757  , 0.23065266,\n",
      "       0.21456619, 0.20477907, 0.22448359, 0.22565907, 0.21601395,\n",
      "       0.2447809 , 0.24103703, 0.24824855, 0.2592045 ]), 'std_train_score': array([0.00236278, 0.00236278, 0.01009008, 0.00774299, 0.00358084,\n",
      "       0.00358084, 0.0047952 , 0.00690409, 0.00369997, 0.00874325,\n",
      "       0.0052627 , 0.00733006, 0.00700493, 0.00874001, 0.01100082,\n",
      "       0.00904111, 0.00308186, 0.0027711 , 0.00880177, 0.01040322,\n",
      "       0.00692986, 0.00377645, 0.00412993, 0.00161111])}\n",
      "Best parameters: {'adaboostclassifier__learning_rate': 1, 'adaboostclassifier__n_estimators': 100}\n",
      "Best Score: 0.2577190324390705\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "ada = AdaBoostClassifier(random_state=42, \n",
    "                         n_estimators=50, \n",
    "                         learning_rate=1.0, \n",
    "                         algorithm='SAMME.R')\n",
    "\n",
    "smote = SMOTE(sampling_strategy = 0.666, random_state=42) #ratio of minority:majority 40:60\n",
    "pipeline = make_pipeline(smote, ada)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "param_ada = {\n",
    "    'adaboostclassifier__learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "    'adaboostclassifier__n_estimators' : [25, 50, 100, 200],\n",
    "    # 'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "ada_tune = HalvingGridSearchCV(estimator=pipeline,\n",
    "                            param_grid=param_ada,\n",
    "                            cv=cv,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42,\n",
    "                            scoring = f2_scorer,\n",
    "                            verbose=2)\n",
    "\n",
    "ada_tune_train = ada_tune.fit(X_train, y_train)\n",
    "\n",
    "import pickle\n",
    "pickle.dump(ada_tune, open('ada_tune_pipeline.pkl','wb'))\n",
    "pickle.dump(ada_tune_train, open('ada_tune_train_pipeline.pkl','wb'))\n",
    "\n",
    "# ada_tune.cv_results_, ada_tune.best_params_, ada_tune.best_score_\n",
    "print(f'CV Results: {ada_tune.cv_results_}')\n",
    "print(f'Best parameters: {ada_tune.best_params_}')\n",
    "print(f'Best Score: {ada_tune.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy_score': 0.9566177876631676,\n",
       " 'classification_report': '              precision    recall  f1-score   support\\n\\n           0       0.99      0.96      0.98    196523\\n           1       0.11      0.40      0.17      2199\\n\\n    accuracy                           0.96    198722\\n   macro avg       0.55      0.68      0.57    198722\\nweighted avg       0.98      0.96      0.97    198722\\n',\n",
       " 'recall_score': 0.4038199181446112,\n",
       " 'precision_score': 0.10831910222005367,\n",
       " 'F2-score': 0.26126868306461104,\n",
       " 'F1-score': 0.1708185053380783,\n",
       " 'average_precision_score': 0.050338566817726235,\n",
       " 'PR-AUC': (array([0.01106571, 0.1083191 , 1.        ]),\n",
       "  array([1.        , 0.40381992, 0.        ]),\n",
       "  array([0, 1], dtype=int64)),\n",
       " 'balanced_accuracy_score': 0.6833116270704535,\n",
       " 'Kappa statistics': 0.15609184295996137,\n",
       " 'TPR': 0.4038199181446112,\n",
       " 'FNR': 0.5961800818553888}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_tune_pred = ada_tune.predict(X_test)\n",
    "res = evaluate_results(y_test, ada_tune_pred)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98    196523\n",
      "           1       0.11      0.40      0.17      2199\n",
      "\n",
      "    accuracy                           0.96    198722\n",
      "   macro avg       0.55      0.68      0.57    198722\n",
      "weighted avg       0.98      0.96      0.97    198722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res['classification_report'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
