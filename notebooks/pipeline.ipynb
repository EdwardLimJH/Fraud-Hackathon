{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read every column except 'device_fraud_count' as its value is a constant 0\n",
    "df = pd.read_csv('Base.csv', usecols=lambda x: x != 'device_fraud_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handle Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with missing values represented by negative values according to documentation\n",
    "missing_features = ['prev_address_months_count', 'current_address_months_count', 'intended_balcon_amount', \n",
    "                    'bank_months_count', 'session_length_in_minutes', 'device_distinct_emails_8w']\n",
    "\n",
    "# Replace negative values with NaN\n",
    "for feature in missing_features:\n",
    "    df[feature] = df[feature].apply(lambda x: x if x >= 0 else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop features with a high percentage of missing values, and have very weak correlation with fraud status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['prev_address_months_count', 'intended_balcon_amount', 'bank_months_count']\n",
    "\n",
    "df.drop(features_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows with missing values as a very small percentage of the remaining observations have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handle Categorical Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform dummy encoding. Very similar to one-hot encoding, but the first encoded column is dropped to reduce correlation between encoded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only features with String data type need to be encoded\n",
    "encoded_features = [feature for feature in df.columns if df[feature].dtype == 'object']\n",
    "\n",
    "df = pd.get_dummies(df, columns=encoded_features, drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the feature matrix and target variable\n",
    "X = df.drop('fraud_bool', axis=1)\n",
    "y = df['fraud_bool']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-Max Scaling (Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EDA, numerical features were identified. Min-max scaling is applied as parametric models are sensitive to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "numeric_features = ['income', 'name_email_similarity', 'current_address_months_count', 'customer_age', 'days_since_request', 'zip_count_4w', 'velocity_6h', 'velocity_24h', \n",
    "                    'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score', 'proposed_credit_limit', 'session_length_in_minutes']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit only on the training data\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Selection - Backward Stepwise (logistic model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanshereen/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tanshereen/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qh/_yq9t4tx45lcx3h89_ggmdm00000gn/T/ipykernel_3838/4206673193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mselected_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_stepwise_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Selected Features:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#35 features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#['income', 'name_email_similarity', 'customer_age', 'zip_count_4w', 'velocity_6h', 'velocity_24h',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/qh/_yq9t4tx45lcx3h89_ggmdm00000gn/T/ipykernel_3838/4206673193.py\u001b[0m in \u001b[0;36mbackward_stepwise_selection\u001b[0;34m(X, y, p_threshold)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mp_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmax_p_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m   1972\u001b[0m     def fit(self, start_params=None, method='newton', maxiter=35,\n\u001b[1;32m   1973\u001b[0m             full_output=1, disp=1, callback=None, **kwargs):\n\u001b[0;32m-> 1974\u001b[0;31m         bnryfit = super().fit(start_params=start_params,\n\u001b[0m\u001b[1;32m   1975\u001b[0m                               \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m                               \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mpass\u001b[0m  \u001b[0;31m# TODO: make a function factory to have multiple call-backs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         mlefit = super().fit(start_params=start_params,\n\u001b[0m\u001b[1;32m    228\u001b[0m                              \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                              \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mwarn_convergence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'warn_convergence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         xopt, retvals, optim_settings = optimizer._fit(f, score, start_params,\n\u001b[0m\u001b[1;32m    520\u001b[0m                                                        \u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                                                        \u001b[0mhessian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/optimizer.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_funcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         xopt, retvals = func(objective, gradient, start_params, fargs, kwargs,\n\u001b[0m\u001b[1;32m    225\u001b[0m                             \u001b[0mdisp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0mretall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/optimizer.py\u001b[0m in \u001b[0;36m_fit_newton\u001b[0;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess, ridge_factor)\u001b[0m\n\u001b[1;32m    413\u001b[0m     while (iterations < maxiter and np.any(np.abs(newparams -\n\u001b[1;32m    414\u001b[0m             oldparams) > tol)):\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;31m# regularize Hessian, not clear what ridge factor should be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# keyword option with absolute default 1e-10, see #1847\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36mhess\u001b[0;34m(params, *args)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mhess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def backward_stepwise_selection(X, y, p_threshold=0.05):\n",
    "    features = X.columns.tolist()\n",
    "    num_features = len(features)\n",
    "    \n",
    "    for i in range(num_features, 0, -1):\n",
    "        model = sm.Logit(y, X[features]).fit()\n",
    "        p_values = model.pvalues\n",
    "        max_p_value = p_values.max()\n",
    "        if max_p_value > p_threshold:\n",
    "            remove_feature = p_values.idxmax()\n",
    "            print(f\"Removing '{remove_feature}' with p-value: {max_p_value:.4f}\")\n",
    "            features.remove(remove_feature)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return features\n",
    "\n",
    "selected_features = backward_stepwise_selection(X_train, y_train)\n",
    "print(\"Selected Features:\", selected_features) #35 features\n",
    "#['income', 'name_email_similarity', 'customer_age', 'zip_count_4w', 'velocity_6h', 'velocity_24h', \n",
    "# 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score', \n",
    "# 'email_is_free', 'phone_home_valid', 'phone_mobile_valid', 'has_other_cards', 'proposed_credit_limit',\n",
    "# 'foreign_request', 'session_length_in_minutes', 'keep_alive_session', 'device_distinct_emails_8w', 'month',\n",
    "# 'payment_type_AC', 'employment_status_CB', 'employment_status_CC', 'employment_status_CD', 'employment_status_CE',\n",
    "# 'employment_status_CF', 'housing_status_BB', 'housing_status_BC', 'housing_status_BD', 'housing_status_BE', 'housing_status_BF',\n",
    "# 'source_TELEAPP', 'device_os_macintosh', 'device_os_windows', 'device_os_x11']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['income', 'name_email_similarity', 'customer_age', 'zip_count_4w', 'velocity_6h', 'velocity_24h', \n",
    "'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score', \n",
    "'email_is_free', 'phone_home_valid', 'phone_mobile_valid', 'has_other_cards', 'proposed_credit_limit',\n",
    "'foreign_request', 'session_length_in_minutes', 'keep_alive_session', 'device_distinct_emails_8w', 'month',\n",
    "'payment_type_AC', 'employment_status_CB', 'employment_status_CC', 'employment_status_CD', 'employment_status_CE',\n",
    "'employment_status_CF', 'housing_status_BB', 'housing_status_BC', 'housing_status_BD', 'housing_status_BE', 'housing_status_BF',\n",
    "'source_TELEAPP', 'device_os_macintosh', 'device_os_windows', 'device_os_x11']\n",
    "\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resampling**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraud class vs non fraud class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of non-fraud class in y: 98.893%\n",
      "% of fraud class in y: 1.107%\n",
      "\n",
      "% of non-fraud class in y_train: 98.893%\n",
      "% of fraud class in y_train: 1.107%\n",
      "\n",
      "% of non-fraud class in y_test: 98.893%\n",
      "% of fraud class in y_test: 1.107%\n"
     ]
    }
   ],
   "source": [
    "ratio = y.value_counts() / len(y) * 100\n",
    "print(f'% of non-fraud class in y: {round(ratio[0],3)}%\\n% of fraud class in y: {round(ratio[1],3)}%\\n')\n",
    "\n",
    "ratio_train = y_train.value_counts() / len(y_train) * 100\n",
    "print(f'% of non-fraud class in y_train: {round(ratio_train[0],3)}%\\n% of fraud class in y_train: {round(ratio_train[1],3)}%\\n')\n",
    "\n",
    "ratio_test = y_test.value_counts() / len(y_test) * 100\n",
    "print(f'% of non-fraud class in y_test: {round(ratio_test[0],3)}%\\n% of fraud class in y_test: {round(ratio_test[1],3)}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of non-fraud class in resampled data: 60.024%\n",
      "% of fraud class in resampled data: 39.976%\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy = 0.666) #ratio of minority:majority 40:60\n",
    "\n",
    "Xt_resampled_SMOTE, yt_resampled_SMOTE = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "ratio_SMOTE = yt_resampled_SMOTE.value_counts() / len(yt_resampled_SMOTE) * 100\n",
    "print(f'% of non-fraud class in resampled data: {round(ratio_SMOTE[0],3)}%\\n% of fraud class in resampled data: {round(ratio_SMOTE[1],3)}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names = ['Ratio of Classes', 'Accuracy', 'Recall','Precision', 'F2 Score', 'F1.5 Score','F1 Score', \n",
    "                 'TPR','FNR', \"PR-AUC\", 'Balanced Accuracy', 'Kappa Statistic']\n",
    "results = pd.DataFrame(index= metrics_names,columns=['Original Dataset', 'SMOTE'])\n",
    "class_reports = {}\n",
    "pr_auc_pts = {}\n",
    "\n",
    "results.loc['Ratio of Classes','Original Dataset'] = str(round(ratio_train,3)[0]) + '% : ' +str(round(ratio_train,3)[1])+'%'\n",
    "results.loc['Ratio of Classes','SMOTE'] = str(round(ratio_SMOTE,3)[0]) + '% : ' +str(round(ratio_SMOTE,3)[1])+'%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, fbeta_score, f1_score, average_precision_score, precision_recall_curve, confusion_matrix,balanced_accuracy_score, cohen_kappa_score\n",
    "def evaluate_results(model,resampler,x_resampled, y_resampled):\n",
    "\n",
    "    model.fit(x_resampled, y_resampled)\n",
    "\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    results.loc['Accuracy',resampler] = accuracy_score(y_test, y_pred_test)\n",
    "    class_reports[resampler] = classification_report(y_test, y_pred_test)\n",
    "    results.loc['Recall',resampler] = recall_score(y_test, y_pred_test)\n",
    "    results.loc['Precision',resampler] = precision_score(y_test, y_pred_test)\n",
    "    results.loc['F2 Score',resampler] = fbeta_score(y_test, y_pred_test, beta =2)\n",
    "    results.loc['F1.5 Score',resampler] = fbeta_score(y_test, y_pred_test, beta =1.5)\n",
    "    results.loc['F1 Score',resampler] = f1_score(y_test, y_pred_test)\n",
    "    results.loc['PR-AUC',resampler] = average_precision_score(y_test, y_pred_test)\n",
    "    pr_auc_pts[resampler] = precision_recall_curve(y_test, y_pred_test)\n",
    "    results.loc['Balanced Accuracy',resampler] = balanced_accuracy_score(y_test, y_pred_test)\n",
    "    results.loc['Kappa Statistic',resampler] = cohen_kappa_score(y_test, y_pred_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_test, labels=[0,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    TPR = TP/(TP+FN)\n",
    "    FNR = FN/(TP+FN)\n",
    "    results.loc['TPR',resampler] = TPR\n",
    "    results.loc['FNR',resampler] = FNR\n",
    "\n",
    "    print(f\"{resampler} Model Performance on Test Data:\")\n",
    "    print(f\"{resampler} Accuracy:\", results.loc['Accuracy',resampler])\n",
    "    print(f\"{resampler} Precision: {results.loc['Precision',resampler]}\")\n",
    "    print(f\"{resampler} Recall: {results.loc['Recall',resampler]}\")\n",
    "    print(f\"{resampler} F2: {results.loc['F2 Score',resampler]}\")\n",
    "    print(f\"{resampler} F1.5: {results.loc['F1.5 Score',resampler]}\")\n",
    "    print(f\"{resampler} F1: {results.loc['F1 Score',resampler]}\")\n",
    "    print(f\"{resampler} PR-AUC: {results.loc['PR-AUC',resampler]}\")\n",
    "    print(f\"{resampler} TPR: {results.loc['TPR',resampler]}\")\n",
    "    print(f\"{resampler} FNR: {results.loc['FNR',resampler]}\")\n",
    "    print(f\"{resampler} Balanced Accuracy: {results.loc['Balanced Accuracy',resampler]}\")\n",
    "    print(f\"{resampler} Kappa Statistic: {results.loc['Kappa Statistic',resampler]}\")\n",
    "    print(f\"{resampler} Classification Report: \\n{class_reports[resampler]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanshereen/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE Model Performance on Test Data:\n",
      "SMOTE Accuracy: 0.9889745473576151\n",
      "SMOTE Precision: 0.6176470588235294\n",
      "SMOTE Recall: 0.009549795361527967\n",
      "SMOTE F2: 0.011891279728199321\n",
      "SMOTE F1.5: 0.013700005018316856\n",
      "SMOTE F1: 0.018808777429467082\n",
      "SMOTE PR-AUC: 0.016858437638643992\n",
      "SMOTE TPR: 0.009549795361527967\n",
      "SMOTE FNR: 0.990450204638472\n",
      "SMOTE Balanced Accuracy: 0.504741822671732\n",
      "SMOTE Kappa Statistic: 0.0184780277014418\n",
      "SMOTE Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    196523\n",
      "           1       0.62      0.01      0.02      2199\n",
      "\n",
      "    accuracy                           0.99    198722\n",
      "   macro avg       0.80      0.50      0.51    198722\n",
      "weighted avg       0.98      0.99      0.98    198722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_results(logistic,\"SMOTE\",X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Performance on Test Data:\n",
      "XGBoost Accuracy: 0.9915963003592959\n",
      "XGBoost Precision: 0.9783001808318263\n",
      "XGBoost Recall: 0.24602091859936334\n",
      "XGBoost F2: 0.2893357578350626\n",
      "XGBoost F1.5: 0.31963823115029766\n",
      "XGBoost F1: 0.39316860465116277\n",
      "XGBoost PR-AUC: 0.2490256229291916\n",
      "XGBoost TPR: 0.24602091859936334\n",
      "XGBoost FNR: 0.7539790814006366\n",
      "XGBoost Balanced Accuracy: 0.6229799285221136\n",
      "XGBoost Kappa Statistic: 0.39045785308260594\n",
      "XGBoost Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    196523\n",
      "           1       0.98      0.25      0.39      2199\n",
      "\n",
      "    accuracy                           0.99    198722\n",
      "   macro avg       0.98      0.62      0.69    198722\n",
      "weighted avg       0.99      0.99      0.99    198722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "evaluate_results(xgb,\"XGBoost\",X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Hyperparameter tuning to improve model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([Xt_resampled_SMOTE, yt_resampled_SMOTE], axis=1)\n",
    "predictors = [x for x in train_df if x not in ['fraud_bool']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=1000, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=20), \n",
    " param_grid = param_test1, scoring='roc_auc',n_jobs=4, cv=5)\n",
    "gsearch1.fit(train_df[predictors],train_df['fraud_bool'])\n",
    "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_\n",
    "#{'max_depth': 9, 'min_child_weight': 1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=1000, max_depth=9,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test3, scoring='roc_auc',n_jobs=4, cv=5)\n",
    "gsearch3.fit(train_df[predictors],train_df['fraud_bool'])\n",
    "gsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_\n",
    "#gamma: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanshereen/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [23:09:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"min_weight_child\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Performance on Test Data:\n",
      "XGBoost Accuracy: 0.9991294371030888\n",
      "XGBoost Precision: 0.9995069033530573\n",
      "XGBoost Recall: 0.9217826284674853\n",
      "XGBoost F2: 0.9363451589061346\n",
      "XGBoost F1.5: 0.9443787406372075\n",
      "XGBoost F1: 0.9590726283416136\n",
      "XGBoost PR-AUC: 0.9221936312856157\n",
      "XGBoost TPR: 0.9217826284674853\n",
      "XGBoost FNR: 0.07821737153251478\n",
      "XGBoost Balanced Accuracy: 0.9608887700022786\n",
      "XGBoost Kappa Statistic: 0.9586333954513937\n",
      "XGBoost Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    196523\n",
      "           1       1.00      0.92      0.96      2199\n",
      "\n",
      "    accuracy                           1.00    198722\n",
      "   macro avg       1.00      0.96      0.98    198722\n",
      "weighted avg       1.00      1.00      1.00    198722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_tuned = XGBClassifier(max_depth=9, min_weight_child = 1, gamma = 0.0,\n",
    " seed=20)\n",
    "evaluate_results(xgb_tuned,\"XGBoost\",X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
